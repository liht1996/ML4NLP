{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Read raw data\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all punctuations by using regex\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "def get_clean_tokens(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    replaced_punctation = list(map(lambda token: re.sub('[^0-9A-Za-z!?]+', '', token), tokens))\n",
    "    #Remove all string == \"\"\n",
    "    removed_punctation = list(filter(lambda token: token, replaced_punctation))\n",
    "    return removed_punctation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess for word2vec\n",
    "#Some document says no need to remove stopwords in word2vec\n",
    "\n",
    "import copy\n",
    "def tokenize_sentences(df):\n",
    "    #Avoid directly modify original dataframe\n",
    "    df_temp = copy.deepcopy(df)\n",
    "    df_temp['text'] = df_temp['text'].str.lower()\n",
    "    df_temp['sentences'] = df_temp['text'].str.split('.')\n",
    "    df_temp['tokenized_sentences'] = list(map(lambda sentences: list(map(get_clean_tokens, sentences)), \n",
    "                                              df_temp.sentences))\n",
    "    #Remove all list == []\n",
    "    df_temp['tokenized_sentences'] = list(map(lambda sentences: list(filter(lambda lst: lst, sentences)),\n",
    "                                         df_temp.tokenized_sentences)) \n",
    "    \n",
    "    return df_temp[['id','label','tokenized_sentences']]\n",
    "    \n",
    "train = tokenize_sentences(train_data)\n",
    "test = tokenize_sentences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train word2vec model\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "train_sentences = [sentence for sentences in train.tokenized_sentences for sentence in sentences]\n",
    "\n",
    "W2Vmodel = Word2Vec(sentences=train_sentences, sg=1, hs=0, workers=4, size=200, min_count=3, window=6,\n",
    "                    sample=1e-3, negative=5, iter=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X train and X validation tensor: (12800, 2039) (3200, 2039)\n",
      "Shape of label train and validation tensor: (12800, 6) (3200, 6)\n"
     ]
    }
   ],
   "source": [
    "#Preprocess for keras\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Split trainset into train and evaluation, proportion of eva is 0.2\n",
    "X_train, X_eva, y_train, y_eva = train_test_split(train_data['text'], train_data['label'], test_size=0.2)\n",
    "\n",
    "\n",
    "#Set a number which is larger than vocab to keep all useful information\n",
    "NUM_WORDS = 80000\n",
    "#keras.preprocessing.text.Tokenizer is different from nltk.word_tokenize, it will turn\n",
    "#each text into either a sequence of integers.\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'', lower=True)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_eva_sequences = tokenizer.texts_to_sequences(X_eva)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "\n",
    "#Make labels to be one-hot for keras input\n",
    "y_train = to_categorical(np.asarray(y_train))\n",
    "y_val = to_categorical(np.asarray(y_eva))\n",
    "#Make input to be same length\n",
    "X_train = pad_sequences(X_train_sequences)\n",
    "X_val = pad_sequences(X_eva_sequences, maxlen=X_train.shape[1])\n",
    "\n",
    "print('Shape of X train and X validation tensor:', X_train.shape,X_val.shape)\n",
    "print('Shape of label train and validation tensor:', y_train.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice: the label of original train set is 1~5 but the y_train is 0~6. However, since no sample is labeled as 0 in train.csv, I believe the influence can be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the embedding layer of NN\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "#Loading previous trained word2vec model\n",
    "word_vectors = W2Vmodel.wv\n",
    "EMBEDDING_DIM=200 #The same as word2vec features\n",
    "\n",
    "vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "\n",
    "#Give embedding matrix value according to word2vec model\n",
    "for word, i in word_index.items():\n",
    "    if i > len(word_index):\n",
    "        continue\n",
    "    try:\n",
    "        embedding_matrix[i] = word_vectors[word]\n",
    "    except KeyError:\n",
    "        #Ignore words not exist in train\n",
    "        embedding_matrix[i] = np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "del(word_vectors)\n",
    "\n",
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(vocabulary_size, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build other layers of NN\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout, concatenate\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "sequence_length = X_train.shape[1]\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 100\n",
    "drop = 0.5\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding = embedding_layer(inputs)\n",
    "reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
    "\n",
    "maxpool=[]\n",
    "for i in range(len(filter_sizes)):\n",
    "    #Convolutional layer is responsible for the convolutional operation in which feature maps identifies features\n",
    "    conv_temp = Conv2D(num_filters, (filter_sizes[i], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "    #Maxpool is used to make the network more flexible to slight changes and decrease the network computationl expenses by extracting \n",
    "    #the group of features that are highly contributing to each feature in the feature maps in the layer.\n",
    "    maxpool_temp = MaxPooling2D((sequence_length - filter_sizes[i] + 1, 1), strides=(1,1))(conv_temp)\n",
    "    maxpool.append(maxpool_temp)\n",
    "\n",
    "merged_tensor = concatenate(maxpool, axis=1)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "reshape = Reshape((3*num_filters,))(flatten)\n",
    "#Dropout is a regulization technique where you turn off part of the network's layers \n",
    "#randomally to increase regulization and hense decrease overfitting. \n",
    "dropout = Dropout(drop)(flatten)\n",
    "#The dense layer is a fully connected layer that comes after the convolutional layers \n",
    "#and they give us the output vector of the Network\n",
    "output = Dense(units=6, activation='softmax', kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
    "\n",
    "model = Model(inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 2039) (3200, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X_val.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/5\n",
      "12800/12800 [==============================] - 1095s 86ms/step - loss: 1.2854 - acc: 0.5439 - val_loss: 1.1247 - val_acc: 0.6031\n",
      "Epoch 2/5\n",
      "12800/12800 [==============================] - 1073s 84ms/step - loss: 1.1031 - acc: 0.6193 - val_loss: 1.0768 - val_acc: 0.6362\n",
      "Epoch 3/5\n",
      "12800/12800 [==============================] - 1095s 86ms/step - loss: 1.0500 - acc: 0.6477 - val_loss: 1.0494 - val_acc: 0.6441\n",
      "Epoch 4/5\n",
      "12800/12800 [==============================] - 1081s 84ms/step - loss: 0.9999 - acc: 0.6733 - val_loss: 1.0552 - val_acc: 0.6406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a393b7ac8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train CNN model\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-3), metrics=['acc'])\n",
    "model.fit(X_train, y_train, epochs=5, verbose=1, validation_data=(X_val, y_val),\n",
    "         callbacks=[EarlyStopping(monitor='val_loss')]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict\n",
    "\n",
    "X_test_sequences = tokenizer.texts_to_sequences(test_data.text)\n",
    "X_test = pad_sequences(X_test_sequences, maxlen=X_train.shape[1])\n",
    "y_pred=model.predict(X_test)\n",
    "\n",
    "y_pred_label = np.argmax(y_pred, axis=1)\n",
    "df_sub = pd.DataFrame()\n",
    "df_sub['id'] = test_data.id\n",
    "df_sub['pred'] = y_pred_label\n",
    "df_sub.to_csv(\"deep_new2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "https://arxiv.org/pdf/1408.5882.pdf\n",
    "\n",
    "https://www.kaggle.com/vukglisovic/classification-combining-lda-and-word2vec\n",
    "\n",
    "https://www.kaggle.com/marijakekic/cnn-in-keras-with-pretrained-word2vec-weights/notebook\n",
    "\n",
    "https://www.kaggle.com/moghazy/beginner-s-guide-to-cnns-with-keras-99-8\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
