{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "stop_words = set(stopwords.words('english') + list(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    https://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize\n",
    "    e.g. \n",
    "    Input: 'It is a nice day. I am happy.'\n",
    "    Output: ['it', 'is', 'a', 'nice', 'day', 'i', 'am', 'happy']\n",
    "    '''\n",
    "    tokens = []\n",
    "    # YOUR CODE HERE\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        word = word.lower()\n",
    "        if word not in stop_words and not word.isnumeric():\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "#     tokens_stem = stem(tokens)\n",
    "#     return tokens_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bigram(tokens):\n",
    "    bigram = []\n",
    "#     tokens=stem(tokens)\n",
    "    for i in range(len(tokens)-1):\n",
    "        big = (tokens[i], tokens[i+1])\n",
    "        bigram.extend(big)\n",
    "    return bigram\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "def stem(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens_stem = [stemmer.stem(token) for token in tokens]\n",
    "    return tokens_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bagofwords(data, vocab_dict):\n",
    "    '''\n",
    "    :param data: a list of words, type: list\n",
    "    :param vocab_dict: a dict from words to indices, type: dict\n",
    "    return a word (sparse) matrix, type: scipy.sparse.csr_matrix\n",
    "    https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html\n",
    "    https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html\n",
    "    '''\n",
    "    data_matrix = sparse.lil_matrix((len(data), len(vocab_dict)))\n",
    "    # YOUR CODE HERE\n",
    "    for i, doc in enumerate(data):\n",
    "        for word in doc:\n",
    "            word_idx = vocab_dict.get(word, -1)\n",
    "            #return -1 if not in dict\n",
    "            if word_idx != -1:\n",
    "                data_matrix[i, word_idx] += 1\n",
    "\n",
    "    data_matrix = data_matrix.tocsr() #to speed up in computation\n",
    "\n",
    "    return data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name, vocab=None):\n",
    "    \"\"\"\n",
    "    https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_name)\n",
    "    df['words'] = df['text'].apply(tokenize)\n",
    "    df['bigrams'] = df['words'].apply(bigram)\n",
    "    \n",
    "#     bigram = set()\n",
    "    if vocab is None:\n",
    "        vocab = set()\n",
    "        for i in range(len(df)):\n",
    "            for word in df.iloc[i]['words']:\n",
    "                vocab.add(word)\n",
    "            for big in df.iloc[i]['bigrams']:\n",
    "                vocab.add(big)\n",
    "                \n",
    "    vocab_dict = dict(zip(vocab, range(len(vocab))))\n",
    "#     bigram_dict = dict(zip(bigram, range(len(bigram))))\n",
    "\n",
    "    data_matrix = get_bagofwords(df['words']+df['bigrams'], vocab_dict)\n",
    "    # import pdb\n",
    "    # pdb.set_trace()\n",
    "\n",
    "    return df['id'], df['label'], data_matrix, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(P):\n",
    "    \"\"\"\n",
    "    normalize P to make sure the sum of every row equals to 1\n",
    "    e.g.\n",
    "    Input: [1,2,1,2,4]\n",
    "    Output: [0.1,0.2,0.1,0.2,0.4] (without laplace smoothing) or [0.1333,0.2,0.1333,0.2,0.3333] (with laplace smoothing)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # with out Laplace smoothing\n",
    "    # norm = np.sum(P, axis = 0, keepdims = True)\n",
    "    # P_ = P / norm\n",
    "\n",
    "    # with Laplace smoothing\n",
    "    K = P.shape[0]\n",
    "    norm = np.sum(P, axis = 0, keepdims = True)\n",
    "    P_ = (P + 1.0) / (norm + K)\n",
    "    \n",
    "    return P_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pre):\n",
    "    assert len(y_true) == len(y_pre)\n",
    "    acc = accuracy_score(y_true, y_pre)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pre, average=\"macro\")\n",
    "    return acc, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 70839\n",
      "Training Set Size: 16000\n",
      "Test Set Size: 4491\n"
     ]
    }
   ],
   "source": [
    "#Read Data\n",
    "\n",
    "train_id_list, train_data_label, train_data_matrix, vocab = read_data(\"train.csv\")\n",
    "print(\"Vocabulary Size:\", len(vocab))\n",
    "print(\"Training Set Size:\", len(train_id_list))\n",
    "test_id_list, _, test_data_matrix, _ = read_data(\"test.csv\", vocab)\n",
    "print(\"Test Set Size:\", len(test_id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "# train_data = transformer.fit_transform(train_data_matrix)\n",
    "# test_data = transformer.fit_transform(test_data_matrix)\n",
    "train_data = train_data_matrix\n",
    "test_data = test_data_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Training Set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_eva, y_train, y_eva = train_test_split(train_data, train_data_label, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import lightgbm as lgb\n",
    "parameters = {'learning_rate':(0.1,0.2,0.3), 'feature_fraction':(0.8,0.9,1), 'bagging_fraction':(0.8,0.9,1), \n",
    "              'max_depth':(3,4,5,6)}\n",
    "lgbmodel=lgb.LGBMClassifier(boosting_type='gbdt')\n",
    "clf = GridSearchCV(lgbmodel,parameters, cv=5, scoring='accuracy')\n",
    "clf.fit(train_data, train_data_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_eva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "\n",
    "import lightgbm as lgb\n",
    "clf=lgb.LGBMClassifier(boosting_type='gbdt', learning_rate=0.2, feature_fraction=0.9, bagging_fraction=0.8)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_eva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evalution: Accuracy: 0.588068\tPrecision: 0.485395\tRecall: 0.534248\tMacro-F1: 0.501397\n"
     ]
    }
   ],
   "source": [
    "#Result on Evaluation Set\n",
    "\n",
    "acc, precision, recall, f1 = evaluate(y_pred, y_eva)\n",
    "    # import pdb; pdb.set_trace()\n",
    "print(\"Evalution: Accuracy: %f\\tPrecision: %f\\tRecall: %f\\tMacro-F1: %f\" % (acc, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "clf=lgb.LGBMClassifier(boosting_type='gbdt', learning_rate=0.2, feature_fraction=0.9, bagging_fraction=0.8)\n",
    "# clf_final=lgb.LGBMClassifier(boosting_type='gbdt', learning_rate=0.3)\n",
    "clf.fit(train_data, train_data_label )\n",
    "pred = clf.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame()\n",
    "sub_df[\"id\"] = test_id_list\n",
    "sub_df['pred'] = pred\n",
    "len(test_id_list)\n",
    "sub_df.to_csv(\"sub_lgb_\"+\"-\"+\"bi\"\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
